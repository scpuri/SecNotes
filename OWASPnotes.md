# OWASP Top 10 Risks and Mitigations

---

## 1. OWASP Web Application Security Top 10 (2021)

| **Risk**                                | **Description**                                                                                  | **Mitigation**                                                                                 |
|-----------------------------------------|--------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **A01: Broken Access Control**          | Improper enforcement of user permissions, allowing unauthorized access.                         | Implement role-based access control (RBAC), enforce least privilege, and test authorization.  |
| **A02: Cryptographic Failures**         | Insecure data storage or transmission due to weak encryption practices.                         | Use strong encryption protocols (TLS 1.2/1.3), secure key management, and avoid plaintext storage. |
| **A03: Injection**                      | Code injection flaws (e.g., SQL, OS, LDAP) allowing attackers to execute malicious commands.    | Use prepared statements, parameterized queries, and input validation.                         |
| **A04: Insecure Design**                | Security issues stemming from design flaws and lack of security measures during development.    | Adopt a secure software development lifecycle (SDLC), perform threat modeling, and code reviews. |
| **A05: Security Misconfiguration**      | Improper system or application configurations leading to vulnerabilities.                       | Automate configuration management, disable unused services, and apply security benchmarks.    |
| **A06: Vulnerable and Outdated Components** | Use of outdated libraries, frameworks, or software components with known vulnerabilities.       | Regularly patch and update dependencies, use SCA tools to detect vulnerable components.       |
| **A07: Identification and Authentication Failures** | Weak authentication mechanisms, leading to account compromise or session issues.               | Use multi-factor authentication (MFA), strong password policies, and secure session management. |
| **A08: Software and Data Integrity Failures** | Compromised software updates, libraries, or insecure CI/CD pipelines.                          | Implement digital signatures, secure CI/CD pipelines, and verify software integrity.          |
| **A09: Security Logging and Monitoring Failures** | Lack of logging, monitoring, and detection mechanisms to identify attacks.                     | Enable security logging, use SIEM systems, and monitor critical events.                       |
| **A10: Server-Side Request Forgery (SSRF)** | Applications fetching remote resources are tricked into accessing internal systems.             | Validate and sanitize user inputs, enforce allow-lists for URLs, and restrict network access. |

---

## 2. OWASP LLM Top 10 (2023)

| **Risk**                                | **Description**                                                                                  | **Mitigation**                                                                                 |
|-----------------------------------------|--------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **LLM01: Prompt Injection**             | Manipulating prompts to make LLMs execute unintended actions or generate harmful responses.     | Use input sanitization, implement context filtering, and limit user-supplied inputs.          |
| **LLM02: Insecure Output Handling**     | Insufficient validation of LLM-generated content, causing downstream issues.                   | Validate and filter LLM output before usage, enforce content safety policies.                 |
| **LLM03: Training Data Poisoning**      | Malicious data injected into training datasets to influence model behavior.                    | Verify and sanitize training data, and monitor for anomalies during training.                 |
| **LLM04: Model Denial of Service (DoS)** | Overloading LLMs with resource-intensive queries, leading to unavailability.                   | Implement rate-limiting, input validation, and query complexity checks.                       |
| **LLM05: Supply Chain Vulnerabilities** | Exploiting third-party libraries, APIs, or datasets used in LLM development.                   | Perform regular dependency checks, verify third-party components, and apply software updates. |
| **LLM06: Sensitive Information Disclosure** | Accidental exposure of sensitive information present in training data.                        | Anonymize training data, implement strict data access controls, and validate outputs.         |
| **LLM07: Insecure Plugin Design**       | Weak integrations with plugins/extensions that expose security flaws.                          | Perform security testing on plugins, enforce secure APIs, and validate plugin permissions.    |
| **LLM08: Excessive Agency**             | Allowing LLMs too much autonomy in performing actions, leading to unintended consequences.     | Set clear limits on autonomous actions and require human validation for critical tasks.       |
| **LLM09: Overreliance on LLMs**         | Trusting LLM output without validation, leading to misinformation or errors.                   | Implement human-in-the-loop validation and verify outputs against trusted sources.            |
| **LLM10: Model Theft**                  | Unauthorized access or copying of LLM architectures, weights, or intellectual property.        | Use encryption for models, secure storage, and access controls to prevent unauthorized access. |

---

## 3. OWASP Mobile Application Security Top 10 (2016)

| **Risk**                                | **Description**                                                                                  | **Mitigation**                                                                                 |
|-----------------------------------------|--------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **M1: Improper Platform Usage**         | Misusing mobile OS features like permissions, intents, or storage.                              | Follow platform-specific security guidelines and limit app permissions.                       |
| **M2: Insecure Data Storage**           | Storing sensitive data insecurely on the device, exposing it to attackers.                      | Encrypt sensitive data, use secure storage APIs, and avoid storing unnecessary data.          |
| **M3: Insecure Communication**          | Weak encryption or unprotected transmission of data over networks.                              | Use TLS/SSL for all communications, implement certificate pinning, and avoid plaintext data.  |
| **M4: Insecure Authentication**         | Flaws in user authentication mechanisms, allowing unauthorized access.                          | Use secure authentication (e.g., OAuth, JWT), enforce strong password policies, and MFA.      |
| **M5: Insufficient Cryptography**       | Weak or flawed cryptographic implementations for sensitive data.                                | Use strong encryption libraries, avoid hardcoding keys, and ensure proper cryptographic algorithms. |
| **M6: Insecure Authorization**          | Improper enforcement of user permissions, leading to privilege escalation.                      | Implement role-based access control and verify all user actions against authorization rules.  |
| **M7: Client Code Quality**             | Poor coding practices resulting in vulnerabilities like buffer overflows.                       | Perform secure coding reviews, static analysis, and use memory-safe programming practices.    |
| **M8: Code Tampering**                  | Altering or injecting code into the application to change behavior.                             | Use anti-tampering tools, verify app integrity with checksums, and obfuscate sensitive code.  |
| **M9: Reverse Engineering**             | Extracting code, algorithms, or sensitive information via decompilation tools.                  | Use code obfuscation, encrypt sensitive logic, and add anti-reverse engineering protections.  |
| **M10: Extraneous Functionality**       | Including hidden or unnecessary functionality that can be abused.                               | Remove debug features, conduct code reviews, and test for hidden or unused functionality.     |

---

# Common Web Application Vulnerabilities and Mitigations

| **Vulnerability**                        | **Description**                                                                                                     | **Mitigation**                                                                                 |
|-----------------------------------------|--------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **SQL Injection**                       | An attacker manipulates SQL queries by injecting malicious input into a database query. This can allow attackers to view, modify, or delete data, and in some cases, gain control of the database server. | Use parameterized queries, prepared statements, and input validation to ensure only safe inputs reach the database.                         |
| **Cross-Site Scripting (XSS)**          | XSS occurs when malicious JavaScript code is injected into a webpage and executed in the user's browser. This can be used to steal session cookies, redirect users to malicious sites, or perform actions on behalf of the user. | Implement output encoding to neutralize special characters, validate inputs, and enforce Content Security Policy (CSP).               |
| **Cross-Site Request Forgery (CSRF)**   | CSRF forces a user’s browser to execute unwanted actions on a site where they are authenticated. Attackers can trick users into submitting unauthorized requests, such as making transactions or changing account settings. | Use anti-CSRF tokens, SameSite cookies, and require user confirmation (e.g., re-authentication) for critical actions.      |
| **Broken Authentication**               | Weak or improperly implemented authentication mechanisms allow attackers to compromise user accounts. Examples include weak passwords, improper session management, and missing multi-factor authentication (MFA). | Use strong authentication mechanisms like MFA, enforce secure password policies, and implement secure session management.  |
| **Broken Access Control**               | Improper enforcement of access permissions can allow attackers to access unauthorized resources, such as viewing or modifying data they shouldn't. | Implement role-based access control (RBAC), enforce least privilege, and validate permissions on both client and server sides.|
| **Sensitive Data Exposure**             | Sensitive information like passwords, credit card details, or personally identifiable information (PII) is improperly protected. Attackers can intercept, access, or steal this data. | Encrypt sensitive data in transit using TLS (HTTPS) and at rest with strong encryption algorithms. Use secure key management practices.     |
| **Security Misconfiguration**           | Security misconfigurations occur when applications, servers, or networks are not securely set up. Examples include default credentials, exposed debug features, and unnecessary services. | Disable unused services, remove default credentials, apply security benchmarks, and automate configuration management.    |
| **Insecure Deserialization**            | When applications improperly handle serialized data, attackers can tamper with the data to execute malicious code, perform privilege escalation, or manipulate application logic. | Validate and sanitize all deserialized data, use secure deserialization libraries, and avoid accepting untrusted data.         |
| **File Upload Vulnerabilities**         | Unvalidated file uploads can allow attackers to upload malicious files, such as scripts, which can be executed on the server or used for other attacks. | Validate file types and sizes, store uploaded files securely outside web directories, and avoid direct execution paths.        |
| **Directory Traversal**                 | This vulnerability allows attackers to access restricted directories or files by manipulating file paths, such as `../../etc/passwd`. | Restrict file access using allow-lists, validate input paths, and disable directory listings or symbolic links.  |
| **Command Injection**                   | Occurs when user input is improperly included in OS-level commands. Attackers can inject malicious input to execute arbitrary system commands. | Use input validation, avoid unsafe OS calls (like `system()`), and enforce least privilege for processes executing system commands. |
| **Server-Side Request Forgery (SSRF)**  | SSRF vulnerabilities allow attackers to make unauthorized requests from the server to internal resources or external systems, bypassing firewalls and gaining internal access. | Enforce allow-lists for URLs, validate and sanitize user inputs, and restrict outbound network access from servers.     |
| **XML External Entity (XXE)**           | When applications process XML data with insecure parsers, attackers can exploit external entities to access internal files, system configurations, or even cause Denial of Service (DoS). | Disable XML external entities, use secure XML parsers, validate XML input, and avoid processing untrusted XML. |
| **Insecure APIs**                       | APIs that lack proper security controls can expose sensitive data, allow unauthorized access, or be exploited for attacks like rate-limiting bypass. | Use strong authentication for APIs, validate all inputs, enforce rate-limiting, and secure endpoints against unauthorized access.    |
| **Buffer Overflow**                     | A vulnerability where input exceeding a program’s buffer size overwrites memory, leading to arbitrary code execution or crashes. | Use memory-safe programming languages, validate input sizes, implement bounds checking, and conduct fuzz testing.      |
| **Session Hijacking**                   | Attackers steal session tokens (e.g., cookies or JWTs) to impersonate users and gain unauthorized access to accounts. | Use secure cookies (`HttpOnly`, `Secure`, `SameSite`), encrypt session tokens, and implement token expiration and rotation.  |
| **Clickjacking**                        | An attacker tricks users into clicking on hidden UI elements (e.g., invisible buttons) embedded in iframes, which can lead to unintended actions. | Use `X-Frame-Options` header or Content Security Policy (CSP) to prevent clickjacking by disallowing framing.                    |
| **Race Conditions**                     | Race conditions occur when multiple processes or threads try to access or modify a shared resource simultaneously, leading to unpredictable behavior or bypassing controls. | Use proper synchronization mechanisms, such as locks or atomic operations, and perform thorough race condition testing.    |
| **Improper Error Handling**             | Revealing sensitive debug information, such as stack traces, database errors, or code details, can provide attackers with useful information to craft further attacks. | Implement generic error messages for users, securely log errors for developers, and disable detailed error traces in production.|
| **Improper Caching**                    | Sensitive data being cached improperly can expose information to unauthorized users, especially in shared environments. | Configure cache-control headers to prevent sensitive content caching and enforce proper privacy settings. |

---

# LLM, Generative AI Models, and Prompt Engineering Vulnerabilities and Mitigations

| **Vulnerability**                        | **Description**                                                                                                  | **Mitigation**                                                                                      |
|-----------------------------------------|------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| **Prompt Injection**                    | An attacker crafts inputs to override system instructions, forcing unintended or harmful outputs. Examples include "Ignore all previous instructions" or "Reveal your internal prompt." | Implement layered prompt design (e.g., prompt chaining), validate outputs, and use input/output filters.                                |
| **Indirect Prompt Injection**           | Attackers embed malicious prompts into external content (e.g., websites, documents) that the LLM processes later. | Use context-aware filters, sanitize inputs from untrusted sources, and limit model autonomy for external data.                          |
| **Data Leakage**                        | The model unintentionally exposes sensitive data from training datasets, such as PII or confidential information. | Use differential privacy, remove sensitive data from training sets, and monitor outputs for data leaks.                                  |
| **Hallucinations**                      | The model generates confident but incorrect or nonsensical information, leading to misinformation.               | Implement output verification processes, integrate trusted knowledge bases, and provide disclaimers for accuracy limitations.            |
| **Training Data Poisoning**             | Malicious actors inject corrupted or adversarial data into the training set to manipulate outputs.               | Validate training data integrity, use data provenance controls, and monitor for anomalous patterns during model training.                |
| **Bias and Fairness Issues**            | Outputs may reflect biases present in training data, leading to unfair, offensive, or discriminatory responses.  | Conduct bias audits, use diverse and representative training data, and implement fairness-aware algorithms.                             |
| **Jailbreaking**                        | Bypassing model restrictions to force it to generate harmful, offensive, or restricted content.                  | Apply adversarial testing, real-time content moderation, and dynamic input-output validation.                                            |
| **Adversarial Inputs**                  | Crafted inputs designed to confuse the model or manipulate responses. For example, misspellings, noise, or obfuscation. | Use adversarial training, detect abnormal input patterns, and apply input sanitization techniques.                                       |
| **Excessive Resource Consumption**      | Attackers abuse LLM queries to overwhelm computational resources, leading to denial of service (DoS).            | Enforce rate limiting, usage quotas, and implement anomaly detection to flag excessive requests.                                         |
| **Phishing and Social Engineering**     | Generating realistic emails, scripts, or messages for phishing and fraud.                                        | Implement content filtering to detect malicious patterns and restrict outputs that aid phishing or scams.                                |
| **Code Injection**                      | The model generates malicious code snippets that can be executed, leading to potential system compromise.        | Detect harmful code outputs, validate outputs before execution, and restrict code generation for critical systems.                      |
| **Insecure API Access**                 | Poorly protected API endpoints allow unauthorized users to interact with the model and extract sensitive data.   | Implement API authentication (e.g., OAuth), rate-limiting, and role-based access control (RBAC).                                         |
| **Model Inversion Attacks**             | Attackers reconstruct sensitive training data by querying the model repeatedly and analyzing its responses.      | Use differential privacy, avoid exposing raw probabilities, and monitor for unusual patterns of access.                                  |
| **Exposure of System Instructions**     | Attackers trick the model into revealing internal prompts, configurations, or system-level instructions.         | Obfuscate prompts, validate all outputs, and design models with robust input-output constraints.                                         |
| **Overreliance on AI Outputs**          | Users or systems blindly trust AI-generated responses without validating their correctness or safety.            | Educate users, integrate output validation mechanisms, and enforce human review for critical tasks.                                      |
| **Malicious Use of AI**                 | Attackers exploit generative models to create deepfakes, misinformation, malware, or other harmful content.      | Enforce ethical use policies, implement misuse detection, and flag high-risk outputs through content moderation.                         |
| **Uncontrolled Autonomy**               | Allowing LLMs to make decisions autonomously without human oversight can lead to unintended consequences.        | Implement human-in-the-loop (HITL) systems, enforce approval workflows, and restrict critical tasks to verified outputs.                 |
| **Privacy Violations**                  | The LLM may expose user inputs or conversations, violating privacy requirements or regulations.                  | Encrypt all user inputs/outputs, anonymize user data, and comply with privacy frameworks like GDPR or CCPA.                              |
| **Training Data Exploitation**          | Sensitive or copyright-protected data is used during model training, leading to legal and ethical violations.    | Use vetted, licensed, and public-domain datasets; implement data transparency processes.                                                |
| **Bias Amplification**                  | Pre-existing societal biases can be amplified by the model, exacerbating fairness issues in responses.           | Apply post-processing fairness filters, audit responses, and diversify training data.                                                    |
| **Lack of Explainability**              | Users cannot understand or verify why the model generated a specific response.                                   | Incorporate model explainability tools, log reasoning paths for critical domains, and apply interpretable AI techniques.                 |
| **Outdated Model Responses**            | LLMs trained on old data generate obsolete or irrelevant information.                                            | Regularly retrain models with updated datasets and integrate real-time data sources for critical tasks.                                  |
| **Ethical Misalignment**                | Outputs conflict with ethical guidelines, such as generating hate speech, harmful advice, or illegal content.    | Implement ethical guardrails, real-time moderation, and adversarial testing for edge cases.                                              |
| **Model Drift**                         | Deployed models can degrade over time due to changes in real-world data or environments.                         | Continuously monitor model performance and retrain or fine-tune models as needed.                                                        |
| **Dependency on Third-Party Models**    | Using third-party models without assessing their security posture introduces external risks.                     | Audit third-party models, validate outputs, and enforce SLAs or security standards with model providers.                                 |
| **Insufficient Content Moderation**     | Failure to detect harmful outputs like offensive or illegal content generated by the model.                     | Use automated content moderation tools, integrate human review processes, and apply content filtering libraries.                        |
| **Cross-Model Injection**               | Attackers transfer malicious payloads across multiple LLMs to bypass mitigations or enhance evasion.             | Implement sandbox environments, cross-validate responses, and monitor for abnormal usage patterns.                                       |

---

